{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ebdb97c-c4f7-4907-bf03-5fc7ab2b4638",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-26 15:38:13.242740: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-09-26 15:38:13.242968: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-09-26 15:38:13.272537: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-09-26 15:38:14.089128: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-09-26 15:38:14.089546: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "import kagglehub\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F  #  to_date, log, count, col\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.rdd import RDD\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import mlflow\n",
    "import mlflow.keras\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d29375c-65ca-4597-9261-9205b229ab52",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATE_COL: str = \"date_col\"\n",
    "RATIO: str = \"ratio\"\n",
    "MIN_SAMPLE_SIZE: int = 1000\n",
    "TRAIN_RATIO: float = 0.7  # how much of the data is for training\n",
    "VALID_RATIO: float = 0.15  # how much is for validation\n",
    "SEQUENCE_LENGTH: int = 21  # the last trade days are in all sequences\n",
    "BATCH_SIZE: int = 32  # Batch size\n",
    "STOCK_NAME: str = \"stock_name\"  # Stock column name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68583421-1904-49d2-a706-7921e528335d",
   "metadata": {},
   "source": [
    "# PySpark & MLflow Demo Notebook Checklist\n",
    "\n",
    "## Setup\n",
    "- [x] Install PySpark and MLflow in the environment\n",
    "- [x] Import necessary Python libraries (pandas, numpy, matplotlib/seaborn)\n",
    "- [x] Initialize Spark session\n",
    "\n",
    "## Data Loading\n",
    "- [x] Load sample dataset (CSV, Parquet, or Delta Lake)\n",
    "- [x] Inspect dataset: `.show()`, `.describe()`, `.printSchema()`\n",
    "- [x] Handle missing values and data types\n",
    "\n",
    "## Data Preprocessing\n",
    "- [x] Select relevant features for modeling\n",
    "- [ ] Perform transformations (scaling, normalization, encoding)\n",
    "- [ ] Split dataset into training and test sets\n",
    "\n",
    "## PySpark Feature Engineering\n",
    "- [ ] Create PySpark DataFrame from preprocessed data\n",
    "- [ ] Apply transformations using PySpark (e.g., `VectorAssembler`, `StandardScaler`)\n",
    "- [ ] Generate new features if needed\n",
    "\n",
    "## Model Training\n",
    "- [ ] Train a simple ML model (e.g., Linear Regression, Random Forest) in PySpark or Scikit-learn\n",
    "- [ ] Evaluate model performance (RMSE, RÂ², accuracy)\n",
    "- [ ] Log model metrics to MLflow\n",
    "\n",
    "## MLflow Tracking\n",
    "- [ ] Start MLflow run (`mlflow.start_run()`)\n",
    "- [ ] Log parameters (learning rate, max depth, etc.)\n",
    "- [ ] Log metrics (RMSE, accuracy, etc.)\n",
    "- [ ] Log trained model (`mlflow.sklearn.log_model` or `mlflow.spark.log_model`)\n",
    "- [ ] End MLflow run\n",
    "\n",
    "## MLflow Model Registry (Optional)\n",
    "- [ ] Register model in MLflow Model Registry\n",
    "- [ ] Promote model to `Staging` or `Production` stage\n",
    "- [ ] Load registered model for inference\n",
    "\n",
    "## Deployment & Inference\n",
    "- [ ] Perform predictions on test set using MLflow model\n",
    "- [ ] Compare predictions vs. actual values\n",
    "- [ ] (Optional) Save predictions to a file or Delta Table\n",
    "\n",
    "## Visualization & Reporting\n",
    "- [ ] Plot feature importance or coefficients\n",
    "- [ ] Plot predicted vs. actual values\n",
    "- [ ] (Optional) Create a simple dashboard in Notebook\n",
    "\n",
    "## Cleanup\n",
    "- [ ] Stop Spark session\n",
    "- [ ] Close MLflow run/session if not already done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ed349b1-4097-4dbd-8092-e49955f8ffc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/09/26 15:38:15 WARN Utils: Your hostname, laptop, resolves to a loopback address: 127.0.1.1; using 192.168.178.43 instead (on interface wlp2s0)\n",
      "25/09/26 15:38:15 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/09/26 15:38:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Datacamp Pyspark Tutorial\")\n",
    "    .config(\"spark.memory.offHeap.enabled\", \"true\")\n",
    "    .config(\"spark.memory.offHeap.size\", \"10g\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e596123-540e-4605-8da6-b65a27f76a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting some test data\n",
    "path: str = kagglehub.dataset_download(\"farukece/semiconductor-stocks-and-the-ai-surge\")\n",
    "file_path: str = os.path.join(path, os.listdir(path)[0])\n",
    "assert os.path.isfile(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9575db85-97b1-4ef3-8a63-5dee7f139080",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df = spark.read.option(\"multiline\", \"true\").csv(file_path, header=True, inferSchema=True)  # ignoring the header warning, this is a Kaggle data set which is a bit off\n",
    "df = df.withColumn(\"date\", F.col(\"date\").cast(\"string\"))\n",
    "df = df.withColumn(DATE_COL, F.to_date(F.col('date'))).filter(F.col(DATE_COL).isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fff31636-c601-467a-a881-a75f9269d118",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_counts = df.groupBy('stock_name').agg(F.count('*').alias('count')).filter(F.col('count') > MIN_SAMPLE_SIZE)\n",
    "df_filtered = df.join(stock_counts, on=\"stock_name\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3365d6b4-4ce7-4c50-a953-f62493a346be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df_filtered.withColumn(DATE_COL, F.to_date(F.col('date'))).filter(F.col(DATE_COL).isNotNull())\n",
    "df_filtered = df_filtered.withColumn(RATIO, F.log(F.col('close')) - F.log(F.col('open')))\n",
    "stock_names = df_filtered.select(STOCK_NAME).distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85256919-492d-451c-b072-25bfa0e379e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_generator(rdd: RDD, seq_length: int = SEQUENCE_LENGTH,\n",
    "                       batch_size: int = BATCH_SIZE):\n",
    "    buffer = deque(maxlen=seq_length + 1)\n",
    "    X_batch, y_batch = [], []\n",
    "    \n",
    "    for row in rdd.toLocalIterator():\n",
    "        val = row[0]\n",
    "        buffer.append(val)\n",
    "        if len(buffer) == seq_length + 1:\n",
    "            X = np.array(list(buffer)[:-1]).reshape(seq_length, 1)\n",
    "            y = np.array([buffer[-1]])\n",
    "            X_batch.append(X)\n",
    "            y_batch.append(y)\n",
    "            if len(X_batch) == batch_size:\n",
    "                yield X_batch, y_batch\n",
    "                X_batch, y_batch = [], []\n",
    "\n",
    "    if X_batch:\n",
    "        yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ac68027-0c8c-4f06-8a49-9695ce1de781",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tf_dataset(generator, seq_length: int = SEQUENCE_LENGTH, batch_size: int = BATCH_SIZE):\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        generator,\n",
    "        output_signature=(\n",
    "            tf.TensorSpec(shape=(None, seq_length, 1), dtype=tf.float32),\n",
    "            tf.TensorSpec(shape=(None, 1), dtype=tf.float32)\n",
    "        )\n",
    "    )\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4fd7f9b6-d2ad-4644-bb64-5aea297d8188",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/26 15:38:20 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , company_name, stock_name, date, open, high, low, close, volume\n",
      " Schema: _c0, company_name, stock_name, date, open, high, low, close, volume\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/dierck/.cache/kagglehub/datasets/farukece/semiconductor-stocks-and-the-ai-surge/versions/1/semi_conductor_se.csv\n",
      "25/09/26 15:38:20 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , company_name, stock_name, date, open, high, low, close, volume\n",
      " Schema: _c0, company_name, stock_name, date, open, high, low, close, volume\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/dierck/.cache/kagglehub/datasets/farukece/semiconductor-stocks-and-the-ai-surge/versions/1/semi_conductor_se.csv\n",
      "25/09/26 15:38:21 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , company_name, stock_name, date, open, high, low, close, volume\n",
      " Schema: _c0, company_name, stock_name, date, open, high, low, close, volume\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/dierck/.cache/kagglehub/datasets/farukece/semiconductor-stocks-and-the-ai-surge/versions/1/semi_conductor_se.csv\n",
      "25/09/26 15:38:21 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , company_name, stock_name, date, open, high, low, close, volume\n",
      " Schema: _c0, company_name, stock_name, date, open, high, low, close, volume\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/dierck/.cache/kagglehub/datasets/farukece/semiconductor-stocks-and-the-ai-surge/versions/1/semi_conductor_se.csv\n",
      "25/09/26 15:38:22 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , company_name, stock_name, date, open, high, low, close, volume\n",
      " Schema: _c0, company_name, stock_name, date, open, high, low, close, volume\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/dierck/.cache/kagglehub/datasets/farukece/semiconductor-stocks-and-the-ai-surge/versions/1/semi_conductor_se.csv\n",
      "25/09/26 15:38:22 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , company_name, stock_name, date, open, high, low, close, volume\n",
      " Schema: _c0, company_name, stock_name, date, open, high, low, close, volume\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/dierck/.cache/kagglehub/datasets/farukece/semiconductor-stocks-and-the-ai-surge/versions/1/semi_conductor_se.csv\n",
      "25/09/26 15:38:22 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , company_name, stock_name, date, open, high, low, close, volume\n",
      " Schema: _c0, company_name, stock_name, date, open, high, low, close, volume\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/dierck/.cache/kagglehub/datasets/farukece/semiconductor-stocks-and-the-ai-surge/versions/1/semi_conductor_se.csv\n",
      "25/09/26 15:38:22 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , company_name, stock_name, date, open, high, low, close, volume\n",
      " Schema: _c0, company_name, stock_name, date, open, high, low, close, volume\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/dierck/.cache/kagglehub/datasets/farukece/semiconductor-stocks-and-the-ai-surge/versions/1/semi_conductor_se.csv\n",
      "25/09/26 15:38:22 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , company_name, stock_name, date, open, high, low, close, volume\n",
      " Schema: _c0, company_name, stock_name, date, open, high, low, close, volume\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/dierck/.cache/kagglehub/datasets/farukece/semiconductor-stocks-and-the-ai-surge/versions/1/semi_conductor_se.csv\n",
      "25/09/26 15:38:22 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , company_name, stock_name, date, open, high, low, close, volume\n",
      " Schema: _c0, company_name, stock_name, date, open, high, low, close, volume\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/dierck/.cache/kagglehub/datasets/farukece/semiconductor-stocks-and-the-ai-surge/versions/1/semi_conductor_se.csv\n",
      "2025-09-26 15:38:23.419767: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m24/24\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0012 - val_loss: 9.7682e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-26 15:38:24.316673: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "/home/dierck/workspace/Demos/Notebooks/.venv/lib/python3.12/site-packages/keras/src/trainers/epoch_iterator.py:164: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n",
      "2025-09-26 15:38:24.483233: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    }
   ],
   "source": [
    "w = Window.partitionBy(STOCK_NAME).orderBy(F.col(DATE_COL))\n",
    "\n",
    "for stock_name in stock_names.rdd.toLocalIterator():\n",
    "    stock = stock_name[STOCK_NAME]\n",
    "    full_data = df_filtered.where(F.col(STOCK_NAME) == stock) \\\n",
    "        .orderBy(F.col(DATE_COL)) \\\n",
    "        .withColumn(\"row_num\", F.row_number().over(w)) \\\n",
    "        .select(F.col(RATIO), F.col(\"row_num\"))\n",
    "    n_samples = full_data.count()\n",
    "    n_train = int(TRAIN_RATIO * n_samples)\n",
    "    n_valid = int(VALID_RATIO * n_samples)\n",
    "    train_data = full_data.where(F.col(\"row_num\") <= n_train).select(RATIO).rdd\n",
    "    val_data = full_data.where((F.col(\"row_num\") > n_train) & (F.col(\"row_num\") <= n_train + n_valid)).rdd\n",
    "    test_data = full_data.where(F.col(\"row_num\") > n_train + n_valid).rdd\n",
    "    train_dataset = to_tf_dataset(lambda: sequence_generator(train_data))\n",
    "    val_dataset = to_tf_dataset(lambda: sequence_generator(val_data))\n",
    "    test_dataset = to_tf_dataset(lambda: sequence_generator(test_data))\n",
    "    # todo create MinMaxScaler\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(SEQUENCE_LENGTH, 1)),\n",
    "        tf.keras.layers.LSTM(4),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        patience=3,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    model.fit(train_dataset,\n",
    "              validation_data=val_dataset,\n",
    "              validation_steps=None,\n",
    "              callbacks=[early_stop])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98aa50d7-babb-4e8d-909d-a6afac226726",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
